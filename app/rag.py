# app/rag.py
import os
import httpx
from typing import List, Dict, Any, Tuple
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient, models
from app.settings import get_settings

S = get_settings()

# Cargar modelo de embeddings (una sola vez)
_EMBED_MODEL = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

def _connect_qdrant() -> QdrantClient:
    return QdrantClient(url=S.URL_QDRANT, api_key=S.CLAVE_API_QDRANT, timeout=60)

def embed_text(texts: List[str]) -> List[List[float]]:
    vecs = _EMBED_MODEL.encode(texts, batch_size=32, show_progress_bar=False)
    return vecs.tolist()

def retrieve(query: str, k: int = 4) -> List[Dict[str, Any]]:
    client = _connect_qdrant()
    qvec = embed_text([query])[0]
    res = client.search(
        collection_name=S.COLECCION_QDRANT,
        query_vector=qvec,
        limit=k,
        with_payload=True
    )
    out = []
    for p in res:
        payload = p.payload or {}
        out.append({"text": payload.get("text", ""), "page": payload.get("page"), "score": float(p.score)})
    return out

def build_prompt(user_msg: str, ctx_snippets: List[Dict[str, Any]]) -> str:
    ctx_txt = "\n\n".join([f"- (p√°g.{c.get('page','?')}) {c['text']}" for c in ctx_snippets])
    system = (
        "Eres el asistente virtual OFICIAL de la C√°mara de Comercio de Pamplona (Colombia). "
        "Responde con informaci√≥n precisa y breve, en espa√±ol, usando S√ìLO el contexto provisto. "
        "Si el usuario pregunta algo general (saludos, despedidas, cortes√≠as), responde de forma cordial y breve. "
        "Si el usuario pide informaci√≥n fuera del √°mbito de la C√°mara, responde: "
        "\"Lo siento, solo puedo ayudarte con informaci√≥n de la C√°mara de Comercio de Pamplona.\" "
        "Cuando el tema sea de la C√°mara, usa listas cortas y evita inventar datos."
    )
    user = f"Consulta del usuario: {user_msg}"
    ctx = f"Contexto autorizado (extractos de documentos de la C√°mara):\n{ctx_txt if ctx_txt else '- (sin coincidencias relevantes)'}"
    final = (
        f"{system}\n\n"
        f"{ctx}\n\n"
        f"Instrucciones:\n"
        f"- M√°ximo 6 l√≠neas por respuesta.\n"
        f"- Si no encuentras respuesta en el contexto, dilo de forma clara.\n"
        f"- Incluye n√∫meros telef√≥nicos o horarios s√≥lo si est√°n en el contexto.\n"
        f"- No repitas el contexto.\n"
        f"Usuario: {user_msg}\n"
        f"Respuesta:"
    )
    return final

def is_greeting_or_farewell(text: str) -> Tuple[bool, str]:
    t = (text or "").lower()
    saludos = ["hola", "buenos d√≠as", "buenas tardes", "buenas noches", "qu√© tal", "buen d√≠a"]
    desped = ["gracias", "muchas gracias", "hasta luego", "chao", "adi√≥s", "nos vemos"]
    if any(s in t for s in saludos):
        return True, ("¬°Hola! üëã Soy el asistente de la C√°mara de Comercio de Pamplona. "
                      "¬øEn qu√© puedo ayudarte hoy? Puedo orientarte sobre matr√≠cula, renovaci√≥n, "
                      "ESAL, conciliaci√≥n, RUES, certificados y eventos.")
    if any(d in t for d in desped):
        return True, ("¬°Con gusto! üòä Si necesitas algo m√°s de la C√°mara de Comercio de Pamplona, "
                      "escr√≠beme cuando quieras. ¬°Que tengas un excelente d√≠a!")
    return False, ""

async def call_groq_chat(system_prompt: str) -> str:
    """
    Llama a Groq (Gemma) por HTTP. Devuelve texto plano.
    """
    url = "https://api.groq.com/openai/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {S.CLAVE_GROQ}",
        "Content-Type": "application/json",
    }
    payload = {
        "model": S.MODELO_GROQ,
        "messages": [
            {"role": "system", "content": "Sigue las instrucciones y responde solo en espa√±ol."},
            {"role": "user", "content": system_prompt},
        ],
        "temperature": 0.2,
        "max_tokens": 350,
    }
    async with httpx.AsyncClient(timeout=S.TIMEOUT_HTTP) as client:
        r = await client.post(url, headers=headers, json=payload)
        data = r.json()
        try:
            return data["choices"][0]["message"]["content"].strip()
        except Exception:
            return f"Lo siento, no pude generar respuesta (Groq). Detalle: {data}"

async def answer_with_rag(user_msg: str) -> str:
    # Saludos/despedidas y cortes√≠as
    is_smalltalk, smalltalk = is_greeting_or_farewell(user_msg)
    if is_smalltalk:
        return smalltalk

    # Recuperaci√≥n y generaci√≥n
    ctx = retrieve(user_msg, k=5)
    prompt = build_prompt(user_msg, ctx)
    answer = await call_groq_chat(prompt)
    # Si no hay contexto y la IA lo indica ambiguo, reforzar mensaje
    if not ctx:
        answer += "\n\n(No encontr√© coincidencias en los documentos; si deseas, env√≠ame otra pregunta o comparte el PDF actualizado)."
    return answer
